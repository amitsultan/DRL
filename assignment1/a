import math
from datetime import datetime
from collections import deque
import gym
import tensorflow as tf
from gym import Env
import numpy as np
from tensorflow.keras.models import clone_model
from tqdm import tqdm
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, BatchNormalization
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import backend as K

current_time = datetime.now().strftime("%Y%m%d-%H%M%S")
train_log_dir = 'logs/' + current_time + '/train'
train_summary_writer = tf.summary.create_file_writer(train_log_dir)


class dqn:

    def __init__(self, env, experience_cap, batch_size, C=2, lr=0.001, epsilon=1):
        self.env = env
        self.observation_space_size = env.observation_space.shape[0]
        self.action_space_size = env.action_space.n
        self.epsilon = epsilon
        self.experience_lst = np.array([], dtype=object)
        self.experience_cap = experience_cap
        self.bath_size = batch_size
        # self.ann = get_ann(input_dim=self.observation_space_size, output_dim=self.action_space_size)
        # self.ann.trainable = True
        # self.target_network = get_ann(input_dim=self.observation_space_size, output_dim=self.action_space_size)
        # self.target_network.trainable = False
        self.ann = None
        self.target_ann_network = None
        self.gamma = 0.97  # discount
        self.C = C
        self.lr_decay_factor = 0.98
        self.learning_rate = lr
        self.min_lr = 1e-10
        self.decaying_rate = 0.9995

    def train(self, episodes, n_steps):
        self.ann = self._build_model()
        self.target_ann_network = clone_model(self.ann)
        self.target_ann_network.set_weights(self.ann.get_weights())
        self.training_model = self._build_training_model()
        optimizer = Adam(learning_rate=self.learning_rate)
        self.training_model.compile(optimizer, loss='mse')
        n_steps_count = 0
        train_rewards = []
        episode = 0
        while np.mean(train_rewards[-100:]) <= 475 or len(train_rewards) == 0:
            episode += 1
            episode_losses = []
            state = self.env.reset()[0]
            rewards = []
            is_done = False
            for step in range(n_steps):
                n_steps_count += 1
                state, action, reward, new_state, is_done, info = self.perform_action(state)
                rewards.append(reward)
                experience_slot = state, action, reward, new_state, is_done, info
                self.insert_memory(experience_slot)
                state = new_state
                if episode >= 5 and len(self.experience_lst) >= self.bath_size:
                    X_train, y_train = self.get_batch(self.bath_size)
                    loss = self.training_model.train_on_batch(x=X_train, y=y_train)
                    episode_losses.append(loss)
                if is_done or step == n_steps - 1:
                    if step == n_steps - 1:
                        exit(-1)
                    with train_summary_writer.as_default():
                        train_rewards.append(np.array(rewards).sum())
                        tf.summary.scalar('loss', np.mean(episode_losses), step=episode)
                        tf.summary.scalar('epsilon', self.epsilon, step=episode)
                        tf.summary.scalar('reward', np.array(rewards).sum(), step=episode)
                        tf.summary.scalar('AVG_REWARD@100', np.mean(train_rewards[-100:]), step=episode)
                        tf.summary.scalar('Learning rate', self.learning_rate, step=episode)
                    break
            # self.epsilon = max(self.epsilon * self.decaying_rate, 0.002)
            self.epsilon = 0.02 + (1 -  0.02) * math.exp(-0.001 * n_steps_count)
            if self.learning_rate > self.min_lr:
                self.learning_rate = self.lr_decay_factor * self.learning_rate
                K.set_value(self.training_model.optimizer.learning_rate, self.learning_rate)
            if (episode + 1) % self.C == 0:
                print('updated target weights')
                self.target_ann_network.set_weights(self.ann.get_weights())

    def insert_memory(self, memory_item):
        if len(self.experience_lst) == self.experience_cap:
            self.experience_lst = np.delete(self.experience_lst, obj=0, axis=1)
        if len(self.experience_lst) == 0:
            self.experience_lst = np.array(memory_item)
        else:
            self.experience_lst = np.vstack([self.experience_lst, memory_item])

    def perform_action(self, state):
        e_rand = np.random.random()
        predictions = self.ann.predict(state.reshape(1, -1))
        if e_rand < self.epsilon:  # exploration
            action = np.random.randint(self.action_space_size)  # random action
        else:  # exploitation
            action = np.argmax(predictions)
        new_state, reward, terminated, truncated, info = self.env.step(action)
        is_done = False
        if terminated or truncated:
            is_done = True
        return state, action, reward, new_state, is_done, info

    def get_batch(self, batch_size):
        index = np.random.choice(self.experience_lst.shape[0], batch_size, replace=False)
        learning_batch = self.experience_lst[index, :]
        states = np.array([row[0] for row in learning_batch])
        actions = np.array([row[1] for row in learning_batch])
        rewards = np.array([row[2] for row in learning_batch])
        new_states = np.array([row[3] for row in learning_batch])
        is_dones = np.array([row[4] for row in learning_batch])
        new_states = np.array(new_states)
        X = np.array(states)
        done_indices = np.where(is_dones)
        undone_indices = np.where(is_dones == 0)
        ys = np.zeros_like(actions, dtype=np.float64)
        ys[done_indices] += rewards[done_indices]
        q_next = np.max(self.target_ann_network.predict(new_states[undone_indices]), axis=-1)
        ys[undone_indices] += rewards[undone_indices] + self.gamma * q_next
        ys = ys
        actions = np.concatenate([np.indices(actions.shape).T, np.expand_dims(actions, axis=0).T], axis=1)
        return [X, actions], ys


    def _build_model(self):
        i = Input(shape=self.observation_space_size)
        layers = [32, 32, 32]
        d = i
        for layer in layers:
            d = Dense(layer, activation='relu')(d)
            d = BatchNormalization()(d)
        o = Dense(self.action_space_size, activation='linear')(d)
        return Model(inputs=i, outputs=o)

    def _build_training_model(self):
        i = Input(shape=self.observation_space_size)
        a = Input(shape=(2,), dtype='int32')
        o = self.ann(i)
        o = tf.gather_nd(o, a)
        return Model(inputs=[i, a], outputs=[o])






env = gym.make('CartPole-v1')
d = dqn(env, experience_cap=50000, batch_size=100, epsilon=1)
d.train(500, 1000)